# The file was automatically generated by Lark v0.6.5
#
#
#   Lark Stand-alone Generator Tool
# ----------------------------------
# Generates a stand-alone LALR(1) parser with a standard lexer
#
# Git:    https://github.com/erezsh/lark
# Author: Erez Shinan (erezshin@gmail.com)
#
#
#    >>> LICENSE
#
#    This tool and its generated code use a separate license from Lark.
#
#    It is licensed under GPLv2 or above.
#
#    If you wish to purchase a commercial license for this tool and its
#    generated code, contact me via email.
#
#    If GPL is incompatible with your free or open-source project,
#    contact me and we'll work it out (for free).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 2 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    See <http://www.gnu.org/licenses/>.
#
#

class LarkError(Exception):
    pass

class GrammarError(LarkError):
    pass

class ParseError(LarkError):
    pass

class LexError(LarkError):
    pass

class UnexpectedInput(LarkError):
    pos_in_stream = None

    def get_context(self, text, span=40):
        pos = self.pos_in_stream
        start = max(pos - span, 0)
        end = pos + span
        before = text[start:pos].rsplit('\n', 1)[-1]
        after = text[pos:end].split('\n', 1)[0]
        return before + after + '\n' + ' ' * len(before) + '^\n'

    def match_examples(self, parse_fn, examples):
        """ Given a parser instance and a dictionary mapping some label with
            some malformed syntax examples, it'll return the label for the
            example that bests matches the current error.
        """
        assert self.state is not None, "Not supported for this exception"

        candidate = None
        for label, example in examples.items():
            assert not isinstance(example, STRING_TYPE)

            for malformed in example:
                try:
                    parse_fn(malformed)
                except UnexpectedInput as ut:
                    if ut.state == self.state:
                        try:
                            if ut.token == self.token:  # Try exact match first
                                return label
                        except AttributeError:
                            pass
                        if not candidate:
                            candidate = label

        return candidate


class UnexpectedCharacters(LexError, UnexpectedInput):
    def __init__(self, seq, lex_pos, line, column, allowed=None, considered_tokens=None, state=None):
        message = "No terminal defined for '%s' at line %d col %d" % (seq[lex_pos], line, column)

        self.line = line
        self.column = column
        self.allowed = allowed
        self.considered_tokens = considered_tokens
        self.pos_in_stream = lex_pos
        self.state = state

        message += '\n\n' + self.get_context(seq)
        if allowed:
            message += '\nExpecting: %s\n' % allowed

        super(UnexpectedCharacters, self).__init__(message)



class UnexpectedToken(ParseError, UnexpectedInput):
    def __init__(self, token, expected, considered_rules=None, state=None):
        self.token = token
        self.expected = expected     # XXX str shouldn't necessary
        self.line = getattr(token, 'line', '?')
        self.column = getattr(token, 'column', '?')
        self.considered_rules = considered_rules
        self.state = state
        self.pos_in_stream = getattr(token, 'pos_in_stream', None)

        message = ("Unexpected token %r at line %s, column %s.\n"
                   "Expected one of: \n\t* %s\n"
                   % (token, self.line, self.column, '\n\t* '.join(self.expected)))

        super(UnexpectedToken, self).__init__(message)


try:
    STRING_TYPE = basestring
except NameError:   # Python 3
    STRING_TYPE = str


import types
from functools import wraps, partial
from contextlib import contextmanager

Str = type(u'')

def smart_decorator(f, create_decorator):
    if isinstance(f, types.FunctionType):
        return wraps(f)(create_decorator(f, True))

    elif isinstance(f, (type, types.BuiltinFunctionType)):
        return wraps(f)(create_decorator(f, False))

    elif isinstance(f, types.MethodType):
        return wraps(f)(create_decorator(f.__func__, True))

    elif isinstance(f, partial):
        # wraps does not work for partials in 2.7: https://bugs.python.org/issue3445
        return create_decorator(f.__func__, True)

    else:
        return create_decorator(f.__func__.__call__, True)



class Meta:
    pass

class Tree(object):
    def __init__(self, data, children, meta=None):
        self.data = data
        self.children = children
        self._meta = meta

    @property
    def meta(self):
        if self._meta is None:
            self._meta = Meta()
        return self._meta

    def __repr__(self):
        return 'Tree(%s, %s)' % (self.data, self.children)

    def _pretty_label(self):
        return self.data

    def _pretty(self, level, indent_str):
        if len(self.children) == 1 and not isinstance(self.children[0], Tree):
            return [ indent_str*level, self._pretty_label(), '\t', '%s' % (self.children[0],), '\n']

        l = [ indent_str*level, self._pretty_label(), '\n' ]
        for n in self.children:
            if isinstance(n, Tree):
                l += n._pretty(level+1, indent_str)
            else:
                l += [ indent_str*(level+1), '%s' % (n,), '\n' ]

        return l

    def pretty(self, indent_str='  '):
        return ''.join(self._pretty(0, indent_str))
    def __eq__(self, other):
        try:
            return self.data == other.data and self.children == other.children
        except AttributeError:
            return False

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        return hash((self.data, tuple(self.children)))

from inspect import getmembers, getmro

class Discard(Exception):
    pass

# Transformers

class Transformer:
    """Visits the tree recursively, starting with the leaves and finally the root (bottom-up)

    Calls its methods (provided by user via inheritance) according to tree.data
    The returned value replaces the old one in the structure.

    Can be used to implement map or reduce.
    """

    def _call_userfunc(self, tree, new_children=None):
        # Assumes tree is already transformed
        children = new_children if new_children is not None else tree.children
        try:
            f = getattr(self, tree.data)
        except AttributeError:
            return self.__default__(tree.data, children, tree.meta)
        else:
            if getattr(f, 'meta', False):
                return f(children, tree.meta)
            elif getattr(f, 'inline', False):
                return f(*children)
            elif getattr(f, 'whole_tree', False):
                if new_children is not None:
                    raise NotImplementedError("Doesn't work with the base Transformer class")
                return f(tree)
            else:
                return f(children)

    def _transform_children(self, children):
        for c in children:
            try:
                yield self._transform_tree(c) if isinstance(c, Tree) else c
            except Discard:
                pass

    def _transform_tree(self, tree):
        children = list(self._transform_children(tree.children))
        return self._call_userfunc(tree, children)

    def transform(self, tree):
        return self._transform_tree(tree)

    def __mul__(self, other):
        return TransformerChain(self, other)

    def __default__(self, data, children, meta):
        "Default operation on tree (for override)"
        return Tree(data, children, meta)

    @classmethod
    def _apply_decorator(cls, decorator, **kwargs):
        mro = getmro(cls)
        assert mro[0] is cls
        libmembers = {name for _cls in mro[1:] for name, _ in getmembers(_cls)}
        for name, value in getmembers(cls):
            if name.startswith('_') or name in libmembers:
                continue

            if isinstance(cls.__dict__[name], (staticmethod, classmethod)):
                kwargs['static'] = True
            setattr(cls, name, decorator(value, **kwargs))
        return cls


class InlineTransformer(Transformer):   # XXX Deprecated
    def _call_userfunc(self, tree, new_children=None):
        # Assumes tree is already transformed
        children = new_children if new_children is not None else tree.children
        try:
            f = getattr(self, tree.data)
        except AttributeError:
            return self.__default__(tree.data, children, tree.meta)
        else:
            return f(*children)


class TransformerChain(object):
    def __init__(self, *transformers):
        self.transformers = transformers

    def transform(self, tree):
        for t in self.transformers:
            tree = t.transform(tree)
        return tree

    def __mul__(self, other):
        return TransformerChain(*self.transformers + (other,))


class Transformer_InPlace(Transformer):
    "Non-recursive. Changes the tree in-place instead of returning new instances"
    def _transform_tree(self, tree):           # Cancel recursion
        return self._call_userfunc(tree)

    def transform(self, tree):
        for subtree in tree.iter_subtrees():
            subtree.children = list(self._transform_children(subtree.children))

        return self._transform_tree(tree)


class Transformer_InPlaceRecursive(Transformer):
    "Recursive. Changes the tree in-place instead of returning new instances"
    def _transform_tree(self, tree):
        tree.children = list(self._transform_children(tree.children))
        return self._call_userfunc(tree)



# Visitors

class VisitorBase:
    def _call_userfunc(self, tree):
        return getattr(self, tree.data, self.__default__)(tree)

    def __default__(self, tree):
        "Default operation on tree (for override)"
        return tree


class Visitor(VisitorBase):
    """Bottom-up visitor, non-recursive

    Visits the tree, starting with the leaves and finally the root (bottom-up)
    Calls its methods (provided by user via inheritance) according to tree.data
    """


    def visit(self, tree):
        for subtree in tree.iter_subtrees():
            self._call_userfunc(subtree)
        return tree

class Visitor_Recursive(VisitorBase):
    """Bottom-up visitor, recursive

    Visits the tree, starting with the leaves and finally the root (bottom-up)
    Calls its methods (provided by user via inheritance) according to tree.data
    """

    def visit(self, tree):
        for child in tree.children:
            if isinstance(child, Tree):
                self.visit(child)

        f = getattr(self, tree.data, self.__default__)
        f(tree)
        return tree



def visit_children_decor(func):
    "See Interpreter"
    @wraps(func)
    def inner(cls, tree):
        values = cls.visit_children(tree)
        return func(cls, values)
    return inner


class Interpreter:
    """Top-down visitor, recursive

    Visits the tree, starting with the root and finally the leaves (top-down)
    Calls its methods (provided by user via inheritance) according to tree.data

    Unlike Transformer and Visitor, the Interpreter doesn't automatically visit its sub-branches.
    The user has to explicitly call visit_children, or use the @visit_children_decor
    """
    def visit(self, tree):
        return getattr(self, tree.data)(tree)

    def visit_children(self, tree):
        return [self.visit(child) if isinstance(child, Tree) else child
                for child in tree.children]

    def __getattr__(self, name):
        return self.__default__

    def __default__(self, tree):
        return self.visit_children(tree)




# Decorators

def _apply_decorator(obj, decorator, **kwargs):
    try:
        _apply = obj._apply_decorator
    except AttributeError:
        return decorator(obj, **kwargs)
    else:
        return _apply(decorator, **kwargs)



def _inline_args__func(func):
    @wraps(func)
    def create_decorator(_f, with_self):
        if with_self:
            def f(self, children):
                return _f(self, *children)
        else:
            def f(self, children):
                return _f(*children)
        return f

    return smart_decorator(func, create_decorator)


def inline_args(obj):   # XXX Deprecated
    return _apply_decorator(obj, _inline_args__func)



def _visitor_args_func_dec(func, inline=False, meta=False, whole_tree=False, static=False):
    assert [whole_tree, meta, inline].count(True) <= 1
    def create_decorator(_f, with_self):
        if with_self:
            def f(self, *args, **kwargs):
                return _f(self, *args, **kwargs)
        else:
            def f(self, *args, **kwargs):
                return _f(*args, **kwargs)
        return f

    if static:
        f = wraps(func)(create_decorator(func, False))
    else:
        f = smart_decorator(func, create_decorator)
    f.inline = inline
    f.meta = meta
    f.whole_tree = whole_tree
    return f

def v_args(inline=False, meta=False, tree=False):
    "A convenience decorator factory, for modifying the behavior of user-supplied visitor methods"
    if [tree, meta, inline].count(True) > 1:
        raise ValueError("Visitor functions can either accept tree, or meta, or be inlined. These cannot be combined.")
    def _visitor_args_dec(obj):
        return _apply_decorator(obj, _visitor_args_func_dec, inline=inline, meta=meta, whole_tree=tree)
    return _visitor_args_dec



class Indenter:
    def __init__(self):
        self.paren_level = 0
        self.indent_level = [0]

    def handle_NL(self, token):
        if self.paren_level > 0:
            return

        yield token

        indent_str = token.rsplit('\n', 1)[1] # Tabs and spaces
        indent = indent_str.count(' ') + indent_str.count('\t') * self.tab_len

        if indent > self.indent_level[-1]:
            self.indent_level.append(indent)
            yield Token.new_borrow_pos(self.INDENT_type, indent_str, token)
        else:
            while indent < self.indent_level[-1]:
                self.indent_level.pop()
                yield Token.new_borrow_pos(self.DEDENT_type, indent_str, token)

            assert indent == self.indent_level[-1], '%s != %s' % (indent, self.indent_level[-1])

    def process(self, stream):
        for token in stream:
            if token.type == self.NL_type:
                for t in self.handle_NL(token):
                    yield t
            else:
                yield token

            if token.type in self.OPEN_PAREN_types:
                self.paren_level += 1
            elif token.type in self.CLOSE_PAREN_types:
                self.paren_level -= 1
                assert self.paren_level >= 0

        while len(self.indent_level) > 1:
            self.indent_level.pop()
            yield Token(self.DEDENT_type, '')

        assert self.indent_level == [0], self.indent_level

    # XXX Hack for ContextualLexer. Maybe there's a more elegant solution?
    @property
    def always_accept(self):
        return (self.NL_type,)


class Token(Str):
    __slots__ = ('type', 'pos_in_stream', 'value', 'line', 'column', 'end_line', 'end_column')

    def __new__(cls, type_, value, pos_in_stream=None, line=None, column=None):
        self = super(Token, cls).__new__(cls, value)
        self.type = type_
        self.pos_in_stream = pos_in_stream
        self.value = value
        self.line = line
        self.column = column
        self.end_line = None
        self.end_column = None
        return self

    @classmethod
    def new_borrow_pos(cls, type_, value, borrow_t):
        return cls(type_, value, borrow_t.pos_in_stream, line=borrow_t.line, column=borrow_t.column)

    def __reduce__(self):
        return (self.__class__, (self.type, self.value, self.pos_in_stream, self.line, self.column, ))

    def __repr__(self):
        return 'Token(%s, %r)' % (self.type, self.value)

    def __deepcopy__(self, memo):
        return Token(self.type, self.value, self.pos_in_stream, self.line, self.column)

    def __eq__(self, other):
        if isinstance(other, Token) and self.type != other.type:
            return False

        return Str.__eq__(self, other)

    __hash__ = Str.__hash__


class LineCounter:
    def __init__(self):
        self.newline_char = '\n'
        self.char_pos = 0
        self.line = 1
        self.column = 1
        self.line_start_pos = 0

    def feed(self, token, test_newline=True):
        """Consume a token and calculate the new line & column.

        As an optional optimization, set test_newline=False is token doesn't contain a newline.
        """
        if test_newline:
            newlines = token.count(self.newline_char)
            if newlines:
                self.line += newlines
                self.line_start_pos = self.char_pos + token.rindex(self.newline_char) + 1

        self.char_pos += len(token)
        self.column = self.char_pos - self.line_start_pos + 1

class _Lex:
    "Built to serve both Lexer and ContextualLexer"
    def __init__(self, lexer, state=None):
        self.lexer = lexer
        self.state = state

    def lex(self, stream, newline_types, ignore_types):
        newline_types = frozenset(newline_types)
        ignore_types = frozenset(ignore_types)
        line_ctr = LineCounter()

        while line_ctr.char_pos < len(stream):
            lexer = self.lexer
            for mre, type_from_index in lexer.mres:
                m = mre.match(stream, line_ctr.char_pos)
                if not m:
                    continue

                t = None
                value = m.group(0)
                type_ = type_from_index[m.lastindex]
                if type_ not in ignore_types:
                    t = Token(type_, value, line_ctr.char_pos, line_ctr.line, line_ctr.column)
                    if t.type in lexer.callback:
                        t = lexer.callback[t.type](t)
                    yield t
                else:
                    if type_ in lexer.callback:
                        t = Token(type_, value, line_ctr.char_pos, line_ctr.line, line_ctr.column)
                        lexer.callback[type_](t)

                line_ctr.feed(value, type_ in newline_types)
                if t:
                    t.end_line = line_ctr.line
                    t.end_column = line_ctr.column

                break
            else:
                raise UnexpectedCharacters(stream, line_ctr.char_pos, line_ctr.line, line_ctr.column, state=self.state)


class UnlessCallback:
    def __init__(self, mres):
        self.mres = mres

    def __call__(self, t):
        for mre, type_from_index in self.mres:
            m = mre.match(t.value)
            if m:
                t.type = type_from_index[m.lastindex]
                break
        return t


from functools import partial, wraps


class ExpandSingleChild:
    def __init__(self, node_builder):
        self.node_builder = node_builder

    def __call__(self, children):
        if len(children) == 1:
            return children[0]
        else:
            return self.node_builder(children)


class PropagatePositions:
    def __init__(self, node_builder):
        self.node_builder = node_builder

    def __call__(self, children):
        res = self.node_builder(children)

        if isinstance(res, Tree):
            res.meta.empty = True

            for c in children:
                if isinstance(c, Tree) and c.children and not c.meta.empty:
                    res.meta.line = c.meta.line
                    res.meta.column = c.meta.column
                    res.meta.start_pos = c.meta.start_pos
                    res.meta.empty = False
                    break
                elif isinstance(c, Token):
                    res.meta.line = c.line
                    res.meta.column = c.column
                    res.meta.start_pos = c.pos_in_stream
                    res.meta.empty = False
                    break

            for c in reversed(children):
                if isinstance(c, Tree) and c.children and not c.meta.empty:
                    res.meta.end_line = c.meta.end_line
                    res.meta.end_column = c.meta.end_column
                    res.meta.end_pos = c.meta.end_pos
                    res.meta.empty = False
                    break
                elif isinstance(c, Token):
                    res.meta.end_line = c.end_line
                    res.meta.end_column = c.end_column
                    res.meta.end_pos = c.pos_in_stream + len(c.value)
                    res.meta.empty = False
                    break

        return res


class ChildFilter:
    def __init__(self, to_include, node_builder):
        self.node_builder = node_builder
        self.to_include = to_include

    def __call__(self, children):
        filtered = []
        for i, to_expand in self.to_include:
            if to_expand:
                filtered += children[i].children
            else:
                filtered.append(children[i])

        return self.node_builder(filtered)

class ChildFilterLALR(ChildFilter):
    "Optimized childfilter for LALR (assumes no duplication in parse tree, so it's safe to change it)"

    def __call__(self, children):
        filtered = []
        for i, to_expand in self.to_include:
            if to_expand:
                if filtered:
                    filtered += children[i].children
                else:   # Optimize for left-recursion
                    filtered = children[i].children
            else:
                filtered.append(children[i])

        return self.node_builder(filtered)

def _should_expand(sym):
    return not sym.is_term and sym.name.startswith('_')

def maybe_create_child_filter(expansion, keep_all_tokens, ambiguous):
    to_include = [(i, _should_expand(sym)) for i, sym in enumerate(expansion)
                  if keep_all_tokens or not (sym.is_term and sym.filter_out)]

    if len(to_include) < len(expansion) or any(to_expand for i, to_expand in to_include):
        return partial(ChildFilter if ambiguous else ChildFilterLALR, to_include)


class Callback(object):
    pass


def ptb_inline_args(func):
    @wraps(func)
    def f(children):
        return func(*children)
    return f



class ParseTreeBuilder:
    def __init__(self, rules, tree_class, propagate_positions=False, keep_all_tokens=False, ambiguous=False):
        self.tree_class = tree_class
        self.propagate_positions = propagate_positions
        self.always_keep_all_tokens = keep_all_tokens
        self.ambiguous = ambiguous

        self.rule_builders = list(self._init_builders(rules))

        self.user_aliases = {}

    def _init_builders(self, rules):
        for rule in rules:
            options = rule.options
            keep_all_tokens = self.always_keep_all_tokens or (options.keep_all_tokens if options else False)
            expand_single_child = options.expand1 if options else False

            wrapper_chain = filter(None, [
                (expand_single_child and not rule.alias) and ExpandSingleChild,
                maybe_create_child_filter(rule.expansion, keep_all_tokens, self.ambiguous),
                self.propagate_positions and PropagatePositions,
            ])

            yield rule, wrapper_chain


    def create_callback(self, transformer=None):
        callback = Callback()

        i = 0
        for rule, wrapper_chain in self.rule_builders:
            internal_callback_name = '_cb%d_%s' % (i, rule.origin)
            i += 1

            user_callback_name = rule.alias or rule.origin.name
            try:
                f = getattr(transformer, user_callback_name)
                assert not getattr(f, 'meta', False), "Meta args not supported for internal transformer"
                # XXX InlineTransformer is deprecated!
                if getattr(f, 'inline', False) or isinstance(transformer, InlineTransformer):
                    f = ptb_inline_args(f)
            except AttributeError:
                f = partial(self.tree_class, user_callback_name)

            self.user_aliases[rule] = rule.alias
            rule.alias = internal_callback_name

            for w in wrapper_chain:
                f = w(f)

            if hasattr(callback, internal_callback_name):
                raise GrammarError("Rule '%s' already exists" % (rule,))
            setattr(callback, internal_callback_name, f)

        return callback



class _Parser:
    def __init__(self, parse_table, callbacks):
        self.states = parse_table.states
        self.start_state = parse_table.start_state
        self.end_state = parse_table.end_state
        self.callbacks = callbacks

    def parse(self, seq, set_state=None):
        token = None
        stream = iter(seq)
        states = self.states

        state_stack = [self.start_state]
        value_stack = []

        if set_state: set_state(self.start_state)

        def get_action(token):
            state = state_stack[-1]
            try:
                return states[state][token.type]
            except KeyError:
                expected = [s for s in states[state].keys() if s.isupper()]
                raise UnexpectedToken(token, expected, state=state)

        def reduce(rule):
            size = len(rule.expansion)
            if size:
                s = value_stack[-size:]
                del state_stack[-size:]
                del value_stack[-size:]
            else:
                s = []

            value = self.callbacks[rule](s)

            _action, new_state = states[state_stack[-1]][rule.origin.name]
            assert _action is Shift
            state_stack.append(new_state)
            value_stack.append(value)

        # Main LALR-parser loop
        for token in stream:
            while True:
                action, arg = get_action(token)
                assert arg != self.end_state

                if action is Shift:
                    state_stack.append(arg)
                    value_stack.append(token)
                    if set_state: set_state(arg)
                    break # next token
                else:
                    reduce(arg)

        token = Token.new_borrow_pos('$END', '', token) if token else Token('$END', '', 0, 1, 1)
        while True:
            _action, arg = get_action(token)
            if _action is Shift:
                assert arg == self.end_state
                val ,= value_stack
                return val
            else:
                reduce(arg)


class Symbol(object):
    is_term = NotImplemented

    def __init__(self, name):
        self.name = name

    def __eq__(self, other):
        assert isinstance(other, Symbol), other
        return self.is_term == other.is_term and self.name == other.name

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        return hash(self.name)

    def __repr__(self):
        return '%s(%r)' % (type(self).__name__, self.name)

    fullrepr = property(__repr__)

class Terminal(Symbol):
    is_term = True

    def __init__(self, name, filter_out=False):
        self.name = name
        self.filter_out = filter_out

    @property
    def fullrepr(self):
        return '%s(%r, %r)' % (type(self).__name__, self.name, self.filter_out)


class NonTerminal(Symbol):
    is_term = False

class Rule(object):
    """
        origin : a symbol
        expansion : a list of symbols
    """
    def __init__(self, origin, expansion, alias=None, options=None):
        self.origin = origin
        self.expansion = expansion
        self.alias = alias
        self.options = options

    def __str__(self):
        return '<%s : %s>' % (self.origin, ' '.join(map(str,self.expansion)))

    def __repr__(self):
        return 'Rule(%r, %r, %r, %r)' % (self.origin, self.expansion, self.alias, self.options)


class RuleOptions:
    def __init__(self, keep_all_tokens=False, expand1=False, priority=None):
        self.keep_all_tokens = keep_all_tokens
        self.expand1 = expand1
        self.priority = priority

    def __repr__(self):
        return 'RuleOptions(%r, %r, %r)' % (
            self.keep_all_tokens,
            self.expand1,
            self.priority,
        )

Shift = 0
Reduce = 1
import re
class LexerRegexps: pass
NEWLINE_TYPES = ['COMMENT', 'STRLIT', 'WS']
IGNORE_TYPES = ['WS', 'COMMENT']
LEXERS = {}
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[0] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[1] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[2] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[3] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n|\\(\\*(.|\n)+\\*\\)))|(?P<WS>(?:[ \t\x0c\r\n])+)',
  {1: 'COMMENT', 3: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[4] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PROGRAM>program)',
  {1: 'COMMENT', 3: 'WS', 4: 'PROGRAM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[5] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[6] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[7] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[8] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[9] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[10] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[11] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<LSQB>\\[)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: '__ANON_0',
   6: 'COMMA',
   7: 'LBRACE',
   8: 'LSQB',
   9: 'RPAR',
   10: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[12] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LSQB>\\[)',
  {1: 'COMMENT', 3: 'WS', 4: 'LSQB'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[13] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n|\\(\\*(.|\n)+\\*\\)))|(?P<WS>(?:[ \t\x0c\r\n])+)',
  {1: 'COMMENT', 3: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[14] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[15] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[16] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[17] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[18] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)',
  {1: 'COMMENT', 3: 'WS', 4: 'INT'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[19] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[20] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[21] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n|\\(\\*(.|\n)+\\*\\)))|(?P<WS>(?:[ \t\x0c\r\n])+)',
  {1: 'COMMENT', 3: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[22] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[23] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LPAR>\\()',
  {1: 'COMMENT', 3: 'WS', 4: 'LPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[24] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COLON>:)|(?P<LPAR>\\()',
  {1: 'COMMENT', 3: 'WS', 4: 'COLON', 5: 'LPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[25] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[26] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'COMMENT', 3: 'STRLIT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[27] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[28] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[29] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[30] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[31] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RSQB>\\])',
  {1: 'COMMENT', 3: 'WS', 4: 'RSQB'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[32] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[33] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COLON>:)',
  {1: 'COMMENT', 3: 'WS', 4: 'COLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[34] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[35] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[36] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[37] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[38] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COLON>:)',
  {1: 'COMMENT', 3: 'WS', 4: 'COLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[39] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[40] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[41] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[42] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[43] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[44] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[45] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[46] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'COMMENT', 3: 'STRLIT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[47] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[48] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<BOOL>bool)|(?P<__ANON_8>int)',
  {1: 'COMMENT', 3: 'WS', 4: 'BOOL', 5: '__ANON_8'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[49] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[50] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[51] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[52] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[53] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[54] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: '__ANON_0',
   6: 'COMMA',
   7: 'LBRACE',
   8: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[55] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)',
  {1: 'COMMENT', 3: 'WS', 4: '__ANON_0'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[56] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)',
  {1: 'COMMENT', 3: 'WS', 4: '__ANON_0'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[57] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)',
  {1: 'COMMENT', 3: 'WS', 4: '__ANON_0'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[58] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[59] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[60] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'COMMENT', 3: 'STRLIT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[61] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[62] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[63] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[64] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[65] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[66] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[67] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[68] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[69] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: '__ANON_0',
   5: 'COMMA',
   6: 'LBRACE',
   7: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[70] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_0>\\->)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_0',
   8: '__ANON_2',
   9: '__ANON_3',
   10: '__ANON_4',
   11: '__ANON_5',
   12: '__ANON_6',
   13: '__ANON_7',
   14: 'COMMA',
   15: 'LBRACE',
   16: 'LESSTHAN',
   17: 'MINUS',
   18: 'MORETHAN',
   19: 'PERCENT',
   20: 'PLUS',
   21: 'RPAR',
   22: 'SEMICOLON',
   23: 'SLASH',
   24: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[71] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[72] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[73] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[74] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[75] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[76] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[77] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MORETHAN>>)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MORETHAN',
   15: 'RPAR',
   16: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[78] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PLUS',
   17: 'RPAR',
   18: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[79] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[80] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<LPAR>\\()',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'INT', 6: 'LPAR'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[81] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[82] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: 'RPAR',
   9: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[83] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LPAR>\\()',
  {1: 'COMMENT', 3: 'WS', 4: 'LPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[84] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: 'RPAR',
   10: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[85] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[86] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[87] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[88] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS',
   9: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[89] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<LPAR>\\()',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'INT', 6: 'LPAR'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[90] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[91] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[92] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[93] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_0>\\->)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<COLON>:)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<LESSTHAN><)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_0',
   8: '__ANON_2',
   9: '__ANON_3',
   10: '__ANON_4',
   11: '__ANON_5',
   12: '__ANON_6',
   13: '__ANON_7',
   14: 'COLON',
   15: 'COMMA',
   16: 'LBRACE',
   17: 'LESSTHAN',
   18: 'LPAR',
   19: 'MINUS',
   20: 'MORETHAN',
   21: 'PERCENT',
   22: 'PLUS',
   23: 'RPAR',
   24: 'SEMICOLON',
   25: 'SLASH',
   26: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[94] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[95] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[96] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[97] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[98] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[99] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[100] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<LPAR>\\()',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'INT', 6: 'LPAR'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[101] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[102] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[103] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[104] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[105] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[106] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[107] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[108] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[109] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[110] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[111] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[112] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[113] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[114] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[115] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[116] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[117] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[118] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[119] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[120] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS',
   9: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[121] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[122] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[123] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[124] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[125] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[126] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[127] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[128] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[129] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS',
   9: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[130] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS',
   9: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[131] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<THEN>then)',
  {1: 'COMMENT', 3: 'WS', 4: 'THEN'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[132] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[133] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[134] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[135] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[136] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PLUS',
   17: 'RPAR',
   18: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[137] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[138] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[139] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: 'RPAR',
   10: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[140] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'ELSE', 5: 'THEN', 6: 'RPAR', 7: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[141] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[142] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MORETHAN>>)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MORETHAN',
   15: 'RPAR',
   16: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[143] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[144] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS',
   9: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[145] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[146] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[147] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[148] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[149] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)',
  {1: 'COMMENT', 3: 'WS', 4: 'ELSE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[150] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[151] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[152] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[153] = (lexer_regexps)
class ContextualLexer:
    def __init__(self):
        self.lexers = LEXERS
        self.set_parser_state(None)
    def set_parser_state(self, state):
        self.parser_state = state
    def lex(self, stream):
        newline_types = NEWLINE_TYPES
        ignore_types = IGNORE_TYPES
        lexers = LEXERS
        l = _Lex(lexers[self.parser_state], self.parser_state)
        for x in l.lex(stream, newline_types, ignore_types):
            yield x
            l.lexer = lexers[self.parser_state]
            l.state = self.parser_state
CON_LEXER = ContextualLexer()
def lex(stream):
    return CON_LEXER.lex(stream)
RULES = {
  0: Rule(NonTerminal('start'), [NonTerminal('type_decls'), NonTerminal('program_decl'), NonTerminal('func_decls')], None, RuleOptions(False, False, None)),
  1: Rule(NonTerminal('type_decls'), [], None, RuleOptions(False, False, None)),
  2: Rule(NonTerminal('type_decls'), [NonTerminal('__anon_star_0')], None, RuleOptions(False, False, None)),
  3: Rule(NonTerminal('type_decl'), [NonTerminal('value_decl')], None, RuleOptions(False, True, None)),
  4: Rule(NonTerminal('type_decl'), [NonTerminal('enum_decl')], None, RuleOptions(False, True, None)),
  5: Rule(NonTerminal('type_decl'), [NonTerminal('enum_set_decl')], None, RuleOptions(False, True, None)),
  6: Rule(NonTerminal('enum_decl'), [Terminal('ENUM', True), NonTerminal('type_name'), NonTerminal('enum_body')], None, RuleOptions(False, False, None)),
  7: Rule(NonTerminal('enum_set_decl'), [Terminal('ENUMSET', True), NonTerminal('type_name'), Terminal('LSQB', True), Terminal('INT', False), Terminal('RSQB', True), NonTerminal('enum_body')], None, RuleOptions(False, False, None)),
  8: Rule(NonTerminal('enum_body'), [Terminal('LBRACE', True), NonTerminal('enum_items'), Terminal('RBRACE', True)], None, RuleOptions(False, True, None)),
  9: Rule(NonTerminal('enum_body'), [Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  10: Rule(NonTerminal('enum_items'), [NonTerminal('enum_item')], None, RuleOptions(False, False, None)),
  11: Rule(NonTerminal('enum_items'), [NonTerminal('enum_item'), NonTerminal('__anon_star_1')], None, RuleOptions(False, False, None)),
  12: Rule(NonTerminal('value_decl'), [Terminal('VALUE', True), NonTerminal('type_name'), NonTerminal('value_body')], None, RuleOptions(False, False, None)),
  13: Rule(NonTerminal('value_body'), [Terminal('LBRACE', True), NonTerminal('value_items'), Terminal('RBRACE', True)], None, RuleOptions(False, True, None)),
  14: Rule(NonTerminal('value_body'), [Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  15: Rule(NonTerminal('value_items'), [NonTerminal('value_item')], None, RuleOptions(False, False, None)),
  16: Rule(NonTerminal('value_items'), [NonTerminal('value_item'), NonTerminal('__anon_star_2')], None, RuleOptions(False, False, None)),
  17: Rule(NonTerminal('value_item'), [NonTerminal('func_name'), Terminal('COLON', True), NonTerminal('expr_type_name'), Terminal('SEMICOLON', True)], None, RuleOptions(False, False, None)),
  18: Rule(NonTerminal('program_decl'), [Terminal('PROGRAM', True), NonTerminal('func_name'), Terminal('LPAR', True), NonTerminal('type_names'), Terminal('RPAR', True), Terminal('__ANON_0', True), NonTerminal('type_name'), Terminal('SEMICOLON', True)], None, RuleOptions(False, False, None)),
  19: Rule(NonTerminal('func_decls'), [], None, RuleOptions(False, False, None)),
  20: Rule(NonTerminal('func_decls'), [NonTerminal('__anon_star_3')], None, RuleOptions(False, False, None)),
  21: Rule(NonTerminal('func_decl'), [Terminal('FUNC', True), NonTerminal('func_name'), Terminal('COLON', True), NonTerminal('func_body'), NonTerminal('func_constraints')], None, RuleOptions(False, False, None)),
  22: Rule(NonTerminal('func_body'), [NonTerminal('func_lhs'), Terminal('__ANON_0', True), NonTerminal('func_rhss')], None, RuleOptions(False, False, None)),
  23: Rule(NonTerminal('func_lhs'), [NonTerminal('opt_arg')], None, RuleOptions(False, True, None)),
  24: Rule(NonTerminal('func_rhss'), [NonTerminal('func_rhs')], None, RuleOptions(False, False, None)),
  25: Rule(NonTerminal('func_rhss'), [NonTerminal('func_rhs'), NonTerminal('__anon_star_4')], None, RuleOptions(False, False, None)),
  26: Rule(NonTerminal('func_rhs'), [NonTerminal('opt_arg')], None, RuleOptions(False, True, None)),
  27: Rule(NonTerminal('opt_arg'), [NonTerminal('type_name')], None, RuleOptions(False, False, None)),
  28: Rule(NonTerminal('opt_arg'), [NonTerminal('type_name'), NonTerminal('var_name')], None, RuleOptions(False, False, None)),
  29: Rule(NonTerminal('func_constraints'), [Terminal('LBRACE', True), NonTerminal('func_constraint_items'), Terminal('RBRACE', True)], None, RuleOptions(False, True, None)),
  30: Rule(NonTerminal('func_constraints'), [Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  31: Rule(NonTerminal('func_constraint_items'), [NonTerminal('func_constraint_item')], None, RuleOptions(False, True, None)),
  32: Rule(NonTerminal('func_constraint_items'), [NonTerminal('func_constraint_item'), NonTerminal('__anon_star_5')], None, RuleOptions(False, True, None)),
  33: Rule(NonTerminal('func_constraint_item'), [NonTerminal('expr'), Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  34: Rule(NonTerminal('expr'), [NonTerminal('imply_expr')], None, RuleOptions(False, True, None)),
  35: Rule(NonTerminal('expr'), [NonTerminal('cond_expr')], None, RuleOptions(False, True, None)),
  36: Rule(NonTerminal('imply_expr'), [NonTerminal('or_expr')], None, RuleOptions(False, True, None)),
  37: Rule(NonTerminal('imply_expr'), [NonTerminal('or_expr'), Terminal('__ANON_1', True), NonTerminal('imply_expr')], None, RuleOptions(False, True, None)),
  38: Rule(NonTerminal('or_expr'), [NonTerminal('or_expr'), Terminal('__ANON_2', True), NonTerminal('and_expr')], None, RuleOptions(False, True, None)),
  39: Rule(NonTerminal('or_expr'), [NonTerminal('and_expr')], None, RuleOptions(False, True, None)),
  40: Rule(NonTerminal('and_expr'), [NonTerminal('and_expr'), Terminal('__ANON_3', True), NonTerminal('cmp_expr')], None, RuleOptions(False, True, None)),
  41: Rule(NonTerminal('and_expr'), [NonTerminal('cmp_expr')], None, RuleOptions(False, True, None)),
  42: Rule(NonTerminal('cmp_expr'), [NonTerminal('term_expr')], None, RuleOptions(False, True, None)),
  43: Rule(NonTerminal('cmp_expr'), [NonTerminal('cmp_expr'), NonTerminal('cmp_op'), NonTerminal('term_expr')], None, RuleOptions(False, True, None)),
  44: Rule(NonTerminal('cmp_op'), [Terminal('MORETHAN', True)], 'expr_gt', RuleOptions(False, True, None)),
  45: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_7', True)], 'expr_ge', RuleOptions(False, True, None)),
  46: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_6', True)], 'expr_le', RuleOptions(False, True, None)),
  47: Rule(NonTerminal('cmp_op'), [Terminal('LESSTHAN', True)], 'expr_lt', RuleOptions(False, True, None)),
  48: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_4', True)], 'expr_eq', RuleOptions(False, True, None)),
  49: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_5', True)], 'expr_ne', RuleOptions(False, True, None)),
  50: Rule(NonTerminal('term_expr'), [NonTerminal('term_expr'), NonTerminal('term_op'), NonTerminal('factor_expr')], None, RuleOptions(False, True, None)),
  51: Rule(NonTerminal('term_expr'), [NonTerminal('factor_expr')], None, RuleOptions(False, True, None)),
  52: Rule(NonTerminal('term_op'), [Terminal('MINUS', True)], 'expr_sub', RuleOptions(False, True, None)),
  53: Rule(NonTerminal('term_op'), [Terminal('PLUS', True)], 'expr_add', RuleOptions(False, True, None)),
  54: Rule(NonTerminal('factor_expr'), [NonTerminal('factor_expr'), NonTerminal('factor_op'), NonTerminal('unary_expr')], None, RuleOptions(False, True, None)),
  55: Rule(NonTerminal('factor_expr'), [NonTerminal('unary_expr')], None, RuleOptions(False, True, None)),
  56: Rule(NonTerminal('factor_op'), [Terminal('STAR', True)], 'expr_mul', RuleOptions(False, True, None)),
  57: Rule(NonTerminal('factor_op'), [Terminal('PERCENT', True)], 'expr_mod', RuleOptions(False, True, None)),
  58: Rule(NonTerminal('factor_op'), [Terminal('SLASH', True)], 'expr_div', RuleOptions(False, True, None)),
  59: Rule(NonTerminal('unary_expr'), [NonTerminal('unary_op'), NonTerminal('atom_expr')], None, RuleOptions(False, True, None)),
  60: Rule(NonTerminal('unary_expr'), [NonTerminal('atom_expr')], None, RuleOptions(False, True, None)),
  61: Rule(NonTerminal('unary_op'), [Terminal('MINUS', True)], 'expr_neg', RuleOptions(False, True, None)),
  62: Rule(NonTerminal('unary_op'), [Terminal('BANG', True)], 'expr_not', RuleOptions(False, True, None)),
  63: Rule(NonTerminal('atom_expr'), [Terminal('LPAR', True), NonTerminal('expr'), Terminal('RPAR', True)], None, RuleOptions(False, True, None)),
  64: Rule(NonTerminal('atom_expr'), [NonTerminal('property_expr')], None, RuleOptions(False, True, None)),
  65: Rule(NonTerminal('atom_expr'), [NonTerminal('const_expr')], None, RuleOptions(False, True, None)),
  66: Rule(NonTerminal('atom_expr'), [NonTerminal('var_expr')], None, RuleOptions(False, True, None)),
  67: Rule(NonTerminal('const_expr'), [Terminal('FALSE', True)], 'expr_false', RuleOptions(False, True, None)),
  68: Rule(NonTerminal('const_expr'), [Terminal('TRUE', True)], 'expr_true', RuleOptions(False, True, None)),
  69: Rule(NonTerminal('const_expr'), [Terminal('INT', False)], 'expr_intlit', RuleOptions(False, True, None)),
  70: Rule(NonTerminal('var_expr'), [NonTerminal('var_name')], 'expr_var', RuleOptions(False, True, None)),
  71: Rule(NonTerminal('cond_expr'), [Terminal('IF', True), NonTerminal('imply_expr'), Terminal('THEN', True), NonTerminal('imply_expr'), Terminal('ELSE', True), NonTerminal('imply_expr')], None, RuleOptions(False, True, None)),
  72: Rule(NonTerminal('property_expr'), [NonTerminal('func_name'), Terminal('LPAR', True), NonTerminal('var_expr'), Terminal('RPAR', True)], None, RuleOptions(False, True, None)),
  73: Rule(NonTerminal('type_names'), [NonTerminal('type_name')], None, RuleOptions(False, False, None)),
  74: Rule(NonTerminal('type_names'), [], None, RuleOptions(False, False, None)),
  75: Rule(NonTerminal('type_names'), [NonTerminal('type_name'), NonTerminal('__anon_star_6')], None, RuleOptions(False, False, None)),
  76: Rule(NonTerminal('enum_item'), [Terminal('STRLIT', False)], None, RuleOptions(False, True, None)),
  77: Rule(NonTerminal('expr_type_name'), [Terminal('BOOL', True)], 'expr_bool', RuleOptions(False, True, None)),
  78: Rule(NonTerminal('expr_type_name'), [Terminal('__ANON_8', True)], 'expr_int', RuleOptions(False, True, None)),
  79: Rule(NonTerminal('type_name'), [Terminal('NAME', False)], None, RuleOptions(False, True, None)),
  80: Rule(NonTerminal('var_name'), [Terminal('NAME', False)], None, RuleOptions(False, True, None)),
  81: Rule(NonTerminal('func_name'), [Terminal('NAME', False)], None, RuleOptions(False, True, None)),
  82: Rule(NonTerminal('__anon_star_0'), [NonTerminal('__anon_star_0'), NonTerminal('type_decl')], None, None),
  83: Rule(NonTerminal('__anon_star_0'), [NonTerminal('type_decl')], None, None),
  84: Rule(NonTerminal('__anon_star_1'), [Terminal('COMMA', True), NonTerminal('enum_item')], None, None),
  85: Rule(NonTerminal('__anon_star_1'), [NonTerminal('__anon_star_1'), Terminal('COMMA', True), NonTerminal('enum_item')], None, None),
  86: Rule(NonTerminal('__anon_star_2'), [NonTerminal('__anon_star_2'), NonTerminal('value_item')], None, None),
  87: Rule(NonTerminal('__anon_star_2'), [NonTerminal('value_item')], None, None),
  88: Rule(NonTerminal('__anon_star_3'), [NonTerminal('func_decl')], None, None),
  89: Rule(NonTerminal('__anon_star_3'), [NonTerminal('__anon_star_3'), NonTerminal('func_decl')], None, None),
  90: Rule(NonTerminal('__anon_star_4'), [NonTerminal('__anon_star_4'), Terminal('COMMA', True), NonTerminal('func_rhs')], None, None),
  91: Rule(NonTerminal('__anon_star_4'), [Terminal('COMMA', True), NonTerminal('func_rhs')], None, None),
  92: Rule(NonTerminal('__anon_star_5'), [NonTerminal('func_constraint_item')], None, None),
  93: Rule(NonTerminal('__anon_star_5'), [NonTerminal('__anon_star_5'), NonTerminal('func_constraint_item')], None, None),
  94: Rule(NonTerminal('__anon_star_6'), [Terminal('COMMA', True), NonTerminal('type_name')], None, None),
  95: Rule(NonTerminal('__anon_star_6'), [NonTerminal('__anon_star_6'), Terminal('COMMA', True), NonTerminal('type_name')], None, None),
}
parse_tree_builder = ParseTreeBuilder(RULES.values(), Tree)
class ParseTable: pass
parse_table = ParseTable()
STATES = {
  0: {0: (1, 1), 1: (0, 1), 2: (0, 2), 3: (0, 3), 4: (0, 4), 5: (0, 5), 6: (0, 6), 7: (0, 7), 8: (0, 8), 9: (0, 9), 10: (0, 10)},
  1: {0: (1, 2), 2: (0, 2), 3: (0, 3), 7: (0, 7), 8: (0, 8), 9: (0, 9), 6: (0, 11), 10: (0, 10)},
  2: {10: (1, 5), 3: (1, 5), 9: (1, 5), 0: (1, 5)},
  3: {11: (0, 12), 12: (0, 13)},
  4: {13: (0, 14)},
  5: {14: (0, 15), 0: (0, 16)},
  6: {10: (1, 83), 0: (1, 83), 9: (1, 83), 3: (1, 83)},
  7: {10: (1, 4), 3: (1, 4), 9: (1, 4), 0: (1, 4)},
  8: {10: (1, 3), 3: (1, 3), 9: (1, 3), 0: (1, 3)},
  9: {11: (0, 12), 12: (0, 17)},
  10: {11: (0, 12), 12: (0, 18)},
  11: {10: (1, 82), 0: (1, 82), 9: (1, 82), 3: (1, 82)},
  12: {15: (1, 79), 16: (1, 79), 17: (1, 79), 18: (1, 79), 19: (1, 79), 11: (1, 79), 20: (1, 79)},
  13: {18: (0, 19)},
  14: {},
  15: {13: (1, 19), 21: (0, 20), 22: (0, 21), 23: (0, 22), 24: (0, 23)},
  16: {25: (0, 24), 11: (0, 25)},
  17: {26: (0, 26), 15: (0, 27), 17: (0, 28)},
  18: {15: (0, 29), 17: (0, 30), 27: (0, 31)},
  19: {28: (0, 32)},
  20: {13: (1, 20), 24: (0, 33), 22: (0, 21)},
  21: {11: (0, 25), 25: (0, 34)},
  22: {13: (1, 0)},
  23: {13: (1, 88), 22: (1, 88)},
  24: {29: (0, 35)},
  25: {30: (1, 81), 29: (1, 81)},
  26: {10: (1, 6), 3: (1, 6), 9: (1, 6), 0: (1, 6)},
  27: {31: (0, 36), 32: (0, 37), 33: (0, 38)},
  28: {10: (1, 9), 3: (1, 9), 9: (1, 9), 0: (1, 9)},
  29: {25: (0, 39), 34: (0, 40), 11: (0, 25), 35: (0, 41)},
  30: {10: (1, 14), 3: (1, 14), 9: (1, 14), 0: (1, 14)},
  31: {10: (1, 12), 3: (1, 12), 9: (1, 12), 0: (1, 12)},
  32: {36: (0, 42)},
  33: {13: (1, 89), 22: (1, 89)},
  34: {30: (0, 43)},
  35: {19: (1, 74), 37: (0, 44), 12: (0, 45), 11: (0, 12)},
  36: {38: (1, 76), 20: (1, 76)},
  37: {38: (1, 10), 39: (0, 46), 20: (0, 47)},
  38: {38: (0, 48)},
  39: {30: (0, 49)},
  40: {38: (0, 50)},
  41: {38: (1, 15), 11: (0, 25), 40: (0, 51), 35: (0, 52), 25: (0, 39)},
  42: {15: (0, 27), 17: (0, 28), 26: (0, 53)},
  43: {41: (0, 54), 12: (0, 55), 42: (0, 56), 43: (0, 57), 11: (0, 12)},
  44: {19: (0, 58)},
  45: {19: (1, 73), 44: (0, 59), 20: (0, 60)},
  46: {38: (1, 11), 20: (0, 61)},
  47: {31: (0, 36), 32: (0, 62)},
  48: {10: (1, 8), 3: (1, 8), 9: (1, 8), 0: (1, 8)},
  49: {45: (0, 63), 46: (0, 64), 47: (0, 65)},
  50: {10: (1, 13), 3: (1, 13), 9: (1, 13), 0: (1, 13)},
  51: {38: (1, 16), 11: (0, 25), 25: (0, 39), 35: (0, 66)},
  52: {38: (1, 87), 11: (1, 87)},
  53: {10: (1, 7), 3: (1, 7), 9: (1, 7), 0: (1, 7)},
  54: {48: (0, 67), 15: (0, 68), 17: (0, 69)},
  55: {15: (1, 27), 16: (1, 27), 20: (1, 27), 17: (1, 27), 49: (0, 70), 11: (0, 71)},
  56: {16: (0, 72)},
  57: {16: (1, 23)},
  58: {16: (0, 73)},
  59: {19: (1, 75), 20: (0, 74)},
  60: {12: (0, 75), 11: (0, 12)},
  61: {32: (0, 76), 31: (0, 36)},
  62: {38: (1, 84), 20: (1, 84)},
  63: {17: (1, 77)},
  64: {17: (1, 78)},
  65: {17: (0, 77)},
  66: {38: (1, 86), 11: (1, 86)},
  67: {13: (1, 21), 22: (1, 21)},
  68: {50: (0, 78), 51: (0, 79), 29: (0, 80), 52: (0, 81), 53: (0, 82), 54: (0, 83), 25: (0, 84), 55: (0, 85), 56: (0, 86), 57: (0, 87), 28: (0, 88), 58: (0, 89), 59: (0, 90), 60: (0, 91), 61: (0, 92), 62: (0, 93), 11: (0, 94), 63: (0, 95), 49: (0, 96), 64: (0, 97), 65: (0, 98), 66: (0, 99), 67: (0, 100), 68: (0, 101), 69: (0, 102), 70: (0, 103)},
  69: {13: (1, 30), 22: (1, 30)},
  70: {15: (1, 28), 16: (1, 28), 20: (1, 28), 17: (1, 28)},
  71: {15: (1, 80), 16: (1, 80), 71: (1, 80), 72: (1, 80), 17: (1, 80), 73: (1, 80), 19: (1, 80), 74: (1, 80), 20: (1, 80), 68: (1, 80), 75: (1, 80), 76: (1, 80), 77: (1, 80), 78: (1, 80), 79: (1, 80), 80: (1, 80), 81: (1, 80), 82: (1, 80), 83: (1, 80), 84: (1, 80), 85: (1, 80)},
  72: {86: (0, 104), 43: (0, 105), 12: (0, 55), 11: (0, 12), 87: (0, 106)},
  73: {11: (0, 12), 12: (0, 107)},
  74: {11: (0, 12), 12: (0, 108)},
  75: {19: (1, 94), 20: (1, 94)},
  76: {38: (1, 85), 20: (1, 85)},
  77: {38: (1, 17), 11: (1, 17)},
  78: {71: (1, 41), 75: (1, 41), 17: (1, 41), 73: (1, 41), 74: (1, 41), 19: (1, 41), 85: (1, 41), 76: (0, 109), 81: (0, 110), 77: (0, 111), 83: (0, 112), 80: (0, 113), 88: (0, 114), 72: (0, 115)},
  79: {71: (1, 42), 72: (1, 42), 17: (1, 42), 73: (1, 42), 74: (1, 42), 19: (1, 42), 83: (1, 42), 75: (1, 42), 76: (1, 42), 77: (1, 42), 85: (1, 42), 80: (1, 42), 81: (1, 42), 68: (0, 116), 84: (0, 117), 89: (0, 118)},
  80: {50: (0, 78), 51: (0, 79), 29: (0, 80), 52: (0, 81), 61: (0, 92), 62: (0, 93), 54: (0, 83), 11: (0, 94), 63: (0, 95), 25: (0, 84), 55: (0, 85), 49: (0, 96), 56: (0, 86), 57: (0, 87), 28: (0, 88), 64: (0, 97), 53: (0, 119), 59: (0, 90), 65: (0, 98), 66: (0, 99), 60: (0, 91), 67: (0, 100), 68: (0, 101), 70: (0, 103)},
  81: {64: (0, 97), 28: (0, 88), 29: (0, 80), 60: (0, 120), 62: (0, 93), 11: (0, 94), 25: (0, 84), 65: (0, 98), 49: (0, 96), 67: (0, 100), 57: (0, 87)},
  82: {17: (0, 121)},
  83: {73: (1, 36), 17: (1, 36), 74: (1, 36), 19: (1, 36), 71: (0, 122), 75: (0, 123)},
  84: {29: (0, 124)},
  85: {71: (1, 39), 75: (1, 39), 17: (1, 39), 73: (1, 39), 74: (1, 39), 19: (1, 39), 85: (0, 125)},
  86: {71: (1, 51), 72: (1, 51), 17: (1, 51), 73: (1, 51), 74: (1, 51), 19: (1, 51), 68: (1, 51), 83: (1, 51), 75: (1, 51), 76: (1, 51), 77: (1, 51), 84: (1, 51), 85: (1, 51), 80: (1, 51), 81: (1, 51), 90: (0, 126), 79: (0, 127), 82: (0, 128), 78: (0, 129)},
  87: {71: (1, 66), 72: (1, 66), 17: (1, 66), 73: (1, 66), 74: (1, 66), 19: (1, 66), 68: (1, 66), 83: (1, 66), 75: (1, 66), 76: (1, 66), 77: (1, 66), 84: (1, 66), 78: (1, 66), 81: (1, 66), 85: (1, 66), 79: (1, 66), 80: (1, 66), 82: (1, 66)},
  88: {71: (1, 69), 72: (1, 69), 17: (1, 69), 73: (1, 69), 74: (1, 69), 19: (1, 69), 68: (1, 69), 75: (1, 69), 76: (1, 69), 77: (1, 69), 78: (1, 69), 79: (1, 69), 80: (1, 69), 82: (1, 69), 81: (1, 69), 83: (1, 69), 84: (1, 69), 85: (1, 69)},
  89: {38: (1, 31), 50: (0, 78), 91: (0, 130), 51: (0, 79), 52: (0, 81), 29: (0, 80), 53: (0, 82), 54: (0, 83), 25: (0, 84), 55: (0, 85), 56: (0, 86), 57: (0, 87), 28: (0, 88), 59: (0, 90), 60: (0, 91), 61: (0, 92), 62: (0, 93), 11: (0, 94), 63: (0, 95), 49: (0, 96), 64: (0, 97), 58: (0, 131), 65: (0, 98), 66: (0, 99), 67: (0, 100), 68: (0, 101), 70: (0, 103)},
  90: {28: (1, 62), 64: (1, 62), 11: (1, 62), 29: (1, 62), 65: (1, 62)},
  91: {71: (1, 60), 72: (1, 60), 17: (1, 60), 73: (1, 60), 74: (1, 60), 19: (1, 60), 68: (1, 60), 83: (1, 60), 75: (1, 60), 76: (1, 60), 77: (1, 60), 84: (1, 60), 78: (1, 60), 81: (1, 60), 85: (1, 60), 79: (1, 60), 80: (1, 60), 82: (1, 60)},
  92: {71: (1, 55), 72: (1, 55), 17: (1, 55), 73: (1, 55), 74: (1, 55), 19: (1, 55), 68: (1, 55), 83: (1, 55), 75: (1, 55), 76: (1, 55), 77: (1, 55), 84: (1, 55), 78: (1, 55), 85: (1, 55), 79: (1, 55), 80: (1, 55), 81: (1, 55), 82: (1, 55)},
  93: {71: (1, 64), 72: (1, 64), 17: (1, 64), 73: (1, 64), 74: (1, 64), 19: (1, 64), 68: (1, 64), 83: (1, 64), 75: (1, 64), 76: (1, 64), 77: (1, 64), 84: (1, 64), 78: (1, 64), 81: (1, 64), 85: (1, 64), 79: (1, 64), 80: (1, 64), 82: (1, 64)},
  94: {15: (1, 80), 16: (1, 80), 71: (1, 80), 72: (1, 80), 17: (1, 80), 73: (1, 80), 19: (1, 80), 74: (1, 80), 20: (1, 80), 68: (1, 80), 75: (1, 80), 76: (1, 80), 77: (1, 80), 78: (1, 80), 79: (1, 80), 80: (1, 80), 81: (1, 80), 82: (1, 80), 83: (1, 80), 84: (1, 80), 85: (1, 80), 30: (1, 81), 29: (1, 81)},
  95: {19: (1, 34), 17: (1, 34)},
  96: {71: (1, 70), 72: (1, 70), 17: (1, 70), 73: (1, 70), 19: (1, 70), 74: (1, 70), 68: (1, 70), 75: (1, 70), 76: (1, 70), 77: (1, 70), 78: (1, 70), 79: (1, 70), 80: (1, 70), 82: (1, 70), 81: (1, 70), 83: (1, 70), 84: (1, 70), 85: (1, 70)},
  97: {71: (1, 67), 72: (1, 67), 17: (1, 67), 73: (1, 67), 74: (1, 67), 19: (1, 67), 68: (1, 67), 75: (1, 67), 76: (1, 67), 77: (1, 67), 78: (1, 67), 79: (1, 67), 80: (1, 67), 82: (1, 67), 81: (1, 67), 83: (1, 67), 84: (1, 67), 85: (1, 67)},
  98: {71: (1, 68), 72: (1, 68), 17: (1, 68), 73: (1, 68), 74: (1, 68), 19: (1, 68), 68: (1, 68), 75: (1, 68), 76: (1, 68), 77: (1, 68), 78: (1, 68), 79: (1, 68), 80: (1, 68), 82: (1, 68), 81: (1, 68), 83: (1, 68), 84: (1, 68), 85: (1, 68)},
  99: {50: (0, 78), 51: (0, 79), 29: (0, 80), 52: (0, 81), 61: (0, 92), 62: (0, 93), 54: (0, 83), 11: (0, 94), 25: (0, 84), 55: (0, 85), 49: (0, 96), 63: (0, 132), 56: (0, 86), 57: (0, 87), 28: (0, 88), 64: (0, 97), 59: (0, 90), 65: (0, 98), 60: (0, 91), 67: (0, 100), 68: (0, 101)},
  100: {71: (1, 65), 72: (1, 65), 17: (1, 65), 73: (1, 65), 74: (1, 65), 19: (1, 65), 68: (1, 65), 83: (1, 65), 75: (1, 65), 76: (1, 65), 77: (1, 65), 84: (1, 65), 78: (1, 65), 81: (1, 65), 85: (1, 65), 79: (1, 65), 80: (1, 65), 82: (1, 65)},
  101: {28: (1, 61), 64: (1, 61), 11: (1, 61), 29: (1, 61), 65: (1, 61)},
  102: {38: (0, 133)},
  103: {19: (1, 35), 17: (1, 35)},
  104: {15: (1, 24), 17: (1, 24), 92: (0, 134), 20: (0, 135)},
  105: {15: (1, 26), 20: (1, 26), 17: (1, 26)},
  106: {15: (1, 22), 17: (1, 22)},
  107: {17: (0, 136)},
  108: {19: (1, 95), 20: (1, 95)},
  109: {28: (1, 44), 59: (1, 44), 64: (1, 44), 11: (1, 44), 29: (1, 44), 68: (1, 44), 65: (1, 44)},
  110: {28: (1, 45), 59: (1, 45), 64: (1, 45), 11: (1, 45), 29: (1, 45), 68: (1, 45), 65: (1, 45)},
  111: {28: (1, 48), 59: (1, 48), 64: (1, 48), 11: (1, 48), 29: (1, 48), 68: (1, 48), 65: (1, 48)},
  112: {28: (1, 46), 59: (1, 46), 64: (1, 46), 11: (1, 46), 29: (1, 46), 68: (1, 46), 65: (1, 46)},
  113: {28: (1, 47), 59: (1, 47), 64: (1, 47), 11: (1, 47), 29: (1, 47), 68: (1, 47), 65: (1, 47)},
  114: {51: (0, 137), 29: (0, 80), 52: (0, 81), 61: (0, 92), 62: (0, 93), 11: (0, 94), 25: (0, 84), 49: (0, 96), 56: (0, 86), 57: (0, 87), 64: (0, 97), 28: (0, 88), 59: (0, 90), 65: (0, 98), 60: (0, 91), 67: (0, 100), 68: (0, 101)},
  115: {28: (1, 49), 59: (1, 49), 64: (1, 49), 11: (1, 49), 29: (1, 49), 68: (1, 49), 65: (1, 49)},
  116: {28: (1, 52), 59: (1, 52), 64: (1, 52), 11: (1, 52), 29: (1, 52), 68: (1, 52), 65: (1, 52)},
  117: {28: (1, 53), 59: (1, 53), 64: (1, 53), 11: (1, 53), 29: (1, 53), 68: (1, 53), 65: (1, 53)},
  118: {29: (0, 80), 52: (0, 81), 61: (0, 92), 62: (0, 93), 11: (0, 94), 25: (0, 84), 49: (0, 96), 56: (0, 138), 57: (0, 87), 64: (0, 97), 28: (0, 88), 59: (0, 90), 65: (0, 98), 60: (0, 91), 67: (0, 100), 68: (0, 101)},
  119: {19: (0, 139)},
  120: {71: (1, 59), 72: (1, 59), 17: (1, 59), 73: (1, 59), 74: (1, 59), 19: (1, 59), 68: (1, 59), 83: (1, 59), 75: (1, 59), 76: (1, 59), 77: (1, 59), 84: (1, 59), 78: (1, 59), 81: (1, 59), 85: (1, 59), 79: (1, 59), 80: (1, 59), 82: (1, 59)},
  121: {28: (1, 33), 59: (1, 33), 64: (1, 33), 11: (1, 33), 66: (1, 33), 68: (1, 33), 29: (1, 33), 38: (1, 33), 65: (1, 33)},
  122: {50: (0, 78), 51: (0, 79), 29: (0, 80), 52: (0, 81), 61: (0, 92), 62: (0, 93), 11: (0, 94), 25: (0, 84), 49: (0, 96), 56: (0, 86), 57: (0, 87), 64: (0, 97), 28: (0, 88), 55: (0, 140), 59: (0, 90), 65: (0, 98), 60: (0, 91), 67: (0, 100), 68: (0, 101)},
  123: {50: (0, 78), 51: (0, 79), 29: (0, 80), 63: (0, 141), 52: (0, 81), 61: (0, 92), 62: (0, 93), 54: (0, 83), 11: (0, 94), 25: (0, 84), 55: (0, 85), 49: (0, 96), 56: (0, 86), 57: (0, 87), 28: (0, 88), 64: (0, 97), 59: (0, 90), 65: (0, 98), 60: (0, 91), 67: (0, 100), 68: (0, 101)},
  124: {49: (0, 96), 11: (0, 71), 57: (0, 142)},
  125: {50: (0, 143), 51: (0, 79), 29: (0, 80), 52: (0, 81), 61: (0, 92), 62: (0, 93), 11: (0, 94), 25: (0, 84), 49: (0, 96), 56: (0, 86), 57: (0, 87), 64: (0, 97), 28: (0, 88), 59: (0, 90), 65: (0, 98), 60: (0, 91), 67: (0, 100), 68: (0, 101)},
  126: {29: (0, 80), 52: (0, 81), 62: (0, 93), 11: (0, 94), 61: (0, 144), 25: (0, 84), 49: (0, 96), 57: (0, 87), 64: (0, 97), 28: (0, 88), 59: (0, 90), 65: (0, 98), 60: (0, 91), 67: (0, 100), 68: (0, 101)},
  127: {28: (1, 57), 59: (1, 57), 64: (1, 57), 11: (1, 57), 29: (1, 57), 68: (1, 57), 65: (1, 57)},
  128: {28: (1, 58), 59: (1, 58), 64: (1, 58), 11: (1, 58), 29: (1, 58), 68: (1, 58), 65: (1, 58)},
  129: {28: (1, 56), 59: (1, 56), 64: (1, 56), 11: (1, 56), 29: (1, 56), 68: (1, 56), 65: (1, 56)},
  130: {38: (1, 32), 50: (0, 78), 51: (0, 79), 29: (0, 80), 52: (0, 81), 53: (0, 82), 54: (0, 83), 25: (0, 84), 55: (0, 85), 56: (0, 86), 57: (0, 87), 28: (0, 88), 59: (0, 90), 60: (0, 91), 61: (0, 92), 62: (0, 93), 11: (0, 94), 63: (0, 95), 49: (0, 96), 64: (0, 97), 58: (0, 145), 65: (0, 98), 66: (0, 99), 67: (0, 100), 68: (0, 101), 70: (0, 103)},
  131: {28: (1, 92), 59: (1, 92), 64: (1, 92), 11: (1, 92), 66: (1, 92), 68: (1, 92), 29: (1, 92), 38: (1, 92), 65: (1, 92)},
  132: {74: (0, 146)},
  133: {13: (1, 29), 22: (1, 29)},
  134: {15: (1, 25), 17: (1, 25), 20: (0, 147)},
  135: {43: (0, 105), 86: (0, 148), 12: (0, 55), 11: (0, 12)},
  136: {13: (1, 18), 22: (1, 18)},
  137: {71: (1, 43), 72: (1, 43), 17: (1, 43), 73: (1, 43), 74: (1, 43), 19: (1, 43), 83: (1, 43), 75: (1, 43), 76: (1, 43), 77: (1, 43), 85: (1, 43), 80: (1, 43), 81: (1, 43), 68: (0, 116), 84: (0, 117), 89: (0, 118)},
  138: {71: (1, 50), 72: (1, 50), 17: (1, 50), 73: (1, 50), 74: (1, 50), 19: (1, 50), 68: (1, 50), 83: (1, 50), 75: (1, 50), 76: (1, 50), 77: (1, 50), 84: (1, 50), 85: (1, 50), 80: (1, 50), 81: (1, 50), 90: (0, 126), 79: (0, 127), 82: (0, 128), 78: (0, 129)},
  139: {71: (1, 63), 72: (1, 63), 17: (1, 63), 73: (1, 63), 74: (1, 63), 19: (1, 63), 68: (1, 63), 83: (1, 63), 75: (1, 63), 76: (1, 63), 77: (1, 63), 84: (1, 63), 78: (1, 63), 81: (1, 63), 85: (1, 63), 79: (1, 63), 80: (1, 63), 82: (1, 63)},
  140: {71: (1, 38), 75: (1, 38), 17: (1, 38), 73: (1, 38), 74: (1, 38), 19: (1, 38), 85: (0, 125)},
  141: {73: (1, 37), 17: (1, 37), 74: (1, 37), 19: (1, 37)},
  142: {19: (0, 149)},
  143: {71: (1, 40), 75: (1, 40), 17: (1, 40), 73: (1, 40), 74: (1, 40), 19: (1, 40), 85: (1, 40), 76: (0, 109), 81: (0, 110), 77: (0, 111), 83: (0, 112), 80: (0, 113), 88: (0, 114), 72: (0, 115)},
  144: {71: (1, 54), 72: (1, 54), 17: (1, 54), 73: (1, 54), 74: (1, 54), 19: (1, 54), 68: (1, 54), 83: (1, 54), 75: (1, 54), 76: (1, 54), 77: (1, 54), 84: (1, 54), 78: (1, 54), 85: (1, 54), 79: (1, 54), 80: (1, 54), 81: (1, 54), 82: (1, 54)},
  145: {28: (1, 93), 59: (1, 93), 64: (1, 93), 11: (1, 93), 66: (1, 93), 68: (1, 93), 29: (1, 93), 38: (1, 93), 65: (1, 93)},
  146: {50: (0, 78), 51: (0, 79), 29: (0, 80), 52: (0, 81), 61: (0, 92), 62: (0, 93), 54: (0, 83), 11: (0, 94), 63: (0, 150), 25: (0, 84), 55: (0, 85), 49: (0, 96), 56: (0, 86), 57: (0, 87), 28: (0, 88), 64: (0, 97), 59: (0, 90), 65: (0, 98), 60: (0, 91), 67: (0, 100), 68: (0, 101)},
  147: {43: (0, 105), 12: (0, 55), 86: (0, 151), 11: (0, 12)},
  148: {15: (1, 91), 20: (1, 91), 17: (1, 91)},
  149: {71: (1, 72), 72: (1, 72), 17: (1, 72), 73: (1, 72), 74: (1, 72), 19: (1, 72), 68: (1, 72), 75: (1, 72), 76: (1, 72), 77: (1, 72), 78: (1, 72), 79: (1, 72), 80: (1, 72), 82: (1, 72), 81: (1, 72), 83: (1, 72), 84: (1, 72), 85: (1, 72)},
  150: {73: (0, 152)},
  151: {15: (1, 90), 20: (1, 90), 17: (1, 90)},
  152: {50: (0, 78), 51: (0, 79), 29: (0, 80), 52: (0, 81), 61: (0, 92), 62: (0, 93), 54: (0, 83), 11: (0, 94), 25: (0, 84), 55: (0, 85), 49: (0, 96), 56: (0, 86), 57: (0, 87), 28: (0, 88), 63: (0, 153), 64: (0, 97), 59: (0, 90), 65: (0, 98), 60: (0, 91), 67: (0, 100), 68: (0, 101)},
  153: {19: (1, 71), 17: (1, 71)},
}
TOKEN_TYPES = (
{0: 'PROGRAM',
 1: '__anon_star_0',
 2: 'enum_set_decl',
 3: 'ENUMSET',
 4: 'start',
 5: 'type_decls',
 6: 'type_decl',
 7: 'enum_decl',
 8: 'value_decl',
 9: 'ENUM',
 10: 'VALUE',
 11: 'NAME',
 12: 'type_name',
 13: '$END',
 14: 'program_decl',
 15: 'LBRACE',
 16: '__ANON_0',
 17: 'SEMICOLON',
 18: 'LSQB',
 19: 'RPAR',
 20: 'COMMA',
 21: '__anon_star_3',
 22: 'FUNC',
 23: 'func_decls',
 24: 'func_decl',
 25: 'func_name',
 26: 'enum_body',
 27: 'value_body',
 28: 'INT',
 29: 'LPAR',
 30: 'COLON',
 31: 'STRLIT',
 32: 'enum_item',
 33: 'enum_items',
 34: 'value_items',
 35: 'value_item',
 36: 'RSQB',
 37: 'type_names',
 38: 'RBRACE',
 39: '__anon_star_1',
 40: '__anon_star_2',
 41: 'func_body',
 42: 'func_lhs',
 43: 'opt_arg',
 44: '__anon_star_6',
 45: 'BOOL',
 46: '__ANON_8',
 47: 'expr_type_name',
 48: 'func_constraints',
 49: 'var_name',
 50: 'cmp_expr',
 51: 'term_expr',
 52: 'unary_op',
 53: 'expr',
 54: 'or_expr',
 55: 'and_expr',
 56: 'factor_expr',
 57: 'var_expr',
 58: 'func_constraint_item',
 59: 'BANG',
 60: 'atom_expr',
 61: 'unary_expr',
 62: 'property_expr',
 63: 'imply_expr',
 64: 'FALSE',
 65: 'TRUE',
 66: 'IF',
 67: 'const_expr',
 68: 'MINUS',
 69: 'func_constraint_items',
 70: 'cond_expr',
 71: '__ANON_2',
 72: '__ANON_5',
 73: 'ELSE',
 74: 'THEN',
 75: '__ANON_1',
 76: 'MORETHAN',
 77: '__ANON_4',
 78: 'STAR',
 79: 'PERCENT',
 80: 'LESSTHAN',
 81: '__ANON_7',
 82: 'SLASH',
 83: '__ANON_6',
 84: 'PLUS',
 85: '__ANON_3',
 86: 'func_rhs',
 87: 'func_rhss',
 88: 'cmp_op',
 89: 'term_op',
 90: 'factor_op',
 91: '__anon_star_5',
 92: '__anon_star_4'}
)
parse_table.states = {s: {TOKEN_TYPES[t]: (a, RULES[x] if a is Reduce else x) for t, (a, x) in acts.items()}
                      for s, acts in STATES.items()}
parse_table.start_state = 0
parse_table.end_state = 14
class Lark_StandAlone:
  def __init__(self, transformer=None, postlex=None):
     callback = parse_tree_builder.create_callback(transformer=transformer)
     callbacks = {rule: getattr(callback, rule.alias or rule.origin, None) for rule in RULES.values()}
     self.parser = _Parser(parse_table, callbacks)
     self.postlex = postlex
  def parse(self, stream):
     tokens = lex(stream)
     sps = CON_LEXER.set_parser_state
     if self.postlex: tokens = self.postlex.process(tokens)
     return self.parser.parse(tokens, sps)
